"""
Database Insert Speed Test

Tests the optimized save_articles() function to verify:
- Batch UPSERT with ON CONFLICT DO NOTHING
- Hash precheck performance
- Rate-limit protection
- Execution timers
- Duplicate handling

Run this before pushing to GitHub to verify all optimizations work correctly.
"""

import asyncio
import sys
import time
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from database import db
from ingest.realtime import fetch_all
from dotenv import load_dotenv

load_dotenv()


async def test_insert_speed():
    """
    Test database insert speed with real RSS feed data.
    
    This test will:
    1. Fetch real articles from RSS feeds
    2. Test first insert (all new)
    3. Test second insert (all duplicates)
    4. Verify hash precheck performance
    5. Verify batch processing logs
    """
    
    print("\n" + "="*70)
    print("ğŸ§ª DATABASE INSERT SPEED TEST")
    print("="*70)
    
    # Initialize database
    print("\nğŸ“Š Step 1: Initialize Database")
    db.init_db()
    await db.create_tables()
    await db.run_migrations()
    print("âœ… Database initialized")
    
    # Fetch real articles from RSS feeds
    print("\nğŸ“Š Step 2: Fetch Real Articles from RSS Feeds")
    print("Fetching articles from configured RSS feeds...")
    start_fetch = time.time()
    articles = await fetch_all()
    fetch_time = time.time() - start_fetch
    
    print(f"âœ… Fetched {len(articles)} articles in {fetch_time:.2f}s")
    
    if not articles:
        print("âŒ No articles fetched. Cannot run test.")
        return
    
    # Test 1: First Insert (All New)
    print("\n" + "-"*70)
    print("ğŸ“Š Test 1: First Insert (All New Articles)")
    print("-"*70)
    print(f"Inserting {len(articles)} articles...")
    
    start_insert1 = time.time()
    inserted_count1 = await db.save_articles(articles)
    insert1_time = time.time() - start_insert1
    
    print(f"\nâœ… Test 1 Complete:")
    print(f"   â€¢ Inserted: {inserted_count1} articles")
    print(f"   â€¢ Time: {insert1_time:.2f}s")
    print(f"   â€¢ Rate: {inserted_count1/insert1_time:.1f} articles/sec")
    
    # Test 2: Second Insert (All Duplicates)
    print("\n" + "-"*70)
    print("ğŸ“Š Test 2: Second Insert (All Duplicates - Should Skip)")
    print("-"*70)
    print(f"Re-inserting same {len(articles)} articles...")
    print("Expected: All should be skipped (existing hashes)")
    
    start_insert2 = time.time()
    inserted_count2 = await db.save_articles(articles)
    insert2_time = time.time() - start_insert2
    
    print(f"\nâœ… Test 2 Complete:")
    print(f"   â€¢ Inserted: {inserted_count2} articles (expected: 0)")
    print(f"   â€¢ Time: {insert2_time:.2f}s")
    print(f"   â€¢ Speedup: {insert1_time/insert2_time:.1f}x faster (hash precheck working)")
    
    # Test 3: Mixed Insert (50% New, 50% Duplicates)
    print("\n" + "-"*70)
    print("ğŸ“Š Test 3: Mixed Insert (50% New, 50% Duplicates)")
    print("-"*70)
    
    # Modify half the articles to create new ones
    mixed_articles = articles.copy()
    half = len(mixed_articles) // 2
    
    for i in range(half):
        # Change hash to make it "new"
        mixed_articles[i]['hash'] = f"modified_{mixed_articles[i]['hash']}"
        mixed_articles[i]['id'] = mixed_articles[i]['id'] + 1000000000000000  # Change ID
    
    print(f"Inserting {len(mixed_articles)} articles ({half} new, {half} duplicates)...")
    
    start_insert3 = time.time()
    inserted_count3 = await db.save_articles(mixed_articles)
    insert3_time = time.time() - start_insert3
    
    print(f"\nâœ… Test 3 Complete:")
    print(f"   â€¢ Inserted: {inserted_count3} articles")
    print(f"   â€¢ Expected: ~{half} articles")
    print(f"   â€¢ Time: {insert3_time:.2f}s")
    print(f"   â€¢ Rate: {inserted_count3/insert3_time:.1f} articles/sec")
    
    # Summary
    print("\n" + "="*70)
    print("ğŸ“Š PERFORMANCE SUMMARY")
    print("="*70)
    
    print(f"\nğŸš€ Fetch Performance:")
    print(f"   â€¢ RSS Feeds: {len(articles)} articles in {fetch_time:.2f}s")
    print(f"   â€¢ Rate: {len(articles)/fetch_time:.1f} articles/sec")
    
    print(f"\nğŸ’¾ Insert Performance:")
    print(f"   â€¢ New Articles: {inserted_count1} in {insert1_time:.2f}s ({inserted_count1/insert1_time:.1f}/sec)")
    print(f"   â€¢ Duplicates: {inserted_count2} in {insert2_time:.2f}s (hash precheck)")
    print(f"   â€¢ Mixed (50/50): {inserted_count3} in {insert3_time:.2f}s ({inserted_count3/insert3_time:.1f}/sec)")
    
    print(f"\nâš¡ Optimization Wins:")
    print(f"   â€¢ Hash Precheck Speedup: {insert1_time/insert2_time:.1f}x faster for duplicates")
    print(f"   â€¢ Batch Processing: 50 items per INSERT")
    print(f"   â€¢ ON CONFLICT DO NOTHING: Database-level deduplication")
    
    print(f"\nâœ… Expected Behavior:")
    if inserted_count2 == 0:
        print("   âœ… Duplicate detection working (0 re-inserts)")
    else:
        print(f"   âš ï¸  Duplicate detection issue ({inserted_count2} re-inserts, expected 0)")
    
    if abs(inserted_count3 - half) < 10:  # Allow small margin
        print(f"   âœ… Mixed insert working (~{half} new inserts)")
    else:
        print(f"   âš ï¸  Mixed insert issue ({inserted_count3} inserts, expected ~{half})")
    
    if insert2_time < insert1_time:
        print(f"   âœ… Hash precheck optimization working ({insert1_time/insert2_time:.1f}x speedup)")
    else:
        print(f"   âš ï¸  Hash precheck not optimizing (slower on duplicates)")
    
    print("\n" + "="*70)
    print("ğŸ‰ TEST COMPLETE - Ready for GitHub Push!")
    print("="*70 + "\n")
    
    # Close database
    await db.close_db()


if __name__ == "__main__":
    print("\nğŸ”¬ Starting Database Insert Speed Test...")
    print("This will test the optimized save_articles() function.")
    
    try:
        asyncio.run(test_insert_speed())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Test interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Test failed with error: {str(e)}")
        import traceback
        traceback.print_exc()
